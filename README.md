# simple-web-scraping

ðŸ“Œ Python Libraries Explanation
import requests


âœ” Allows you to send HTTP requests to websites (GET, POST, etc.).
âœ” Used to download a webpageâ€™s HTML for scraping.

from bs4 import BeautifulSoup


âœ” Comes from the BeautifulSoup4 library.
âœ” Used to parse and extract data from HTML (e.g., tables, tags, text).

import csv  # New library for handling CSV files


âœ” Pythonâ€™s built-in module.
âœ” Used to create and write data into CSV files (Excel-like format).
âœ” Helpful to store scraped data for analysis.

The headers in web scraping are used to make your request look like a real browser. Many websites block bots or scripts, so sending headers makes the website think a real human is visiting.

âœ” What these two lines do
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, "html.parser")

ðŸ”¹ 1. response = requests.get(url, headers=headers)

This sends a request to the website (url).

The website replies with the HTML code.

That reply is stored in response.

headers help to pretend like a normal browser, so website doesn't block your request.

So this line downloads the HTML page.

ðŸ”¹ 2. soup = BeautifulSoup(response.text, "html.parser")

response.text contains the raw HTML code of that page.

BeautifulSoup takes that HTML and converts it into structured data, so we can search easily (find table, headings, links, etc.).

"html.parser" tells BeautifulSoup which parser to use.

This line parses the HTML for web scraping.
